{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Your First Neural Net in PyTorch\n",
    "\n",
    "* For this task, I'll use a an example text dataset about baseball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data. \n",
    "* Before making the neural net, we need to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is stored as pickle file\n",
    "import pickle\n",
    "import pandas as pd\n",
    "with open('data/tfidf_1000.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is a dictionary with 4 fields and 582 members in each field\n",
    "# raw - emails\n",
    "# data - term frequency encoding\n",
    "# target - 1 if about baseball, 0 otherwise\n",
    "# features - key for the encoding\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check out 1 example\n",
    "print(data['raw'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearly not about baseball, so its target should be 0\n",
    "print(data['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the encoded version\n",
    "encoded_data = data['data'][0]\n",
    "encoded_ser = pd.Series(encoded_data, index = data['features'])\n",
    "encoded_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task: Using Neural Networks, predict whether a document is about baseball based on its term-frequency encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task : Use Neural Networks to use X to Predict y.\n",
    "\n",
    "<img src=\"data/pic.png\">\n",
    "\n",
    "* Lets build the neural network above.\n",
    "* Ours will have one big difference though. What is it?\n",
    "* What does the output layer represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network in PyTorch\n",
    "* We'll build up piece by piece.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the neccessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Nets in PyTorch are classes that you have to make.\n",
    "#This is the class definition\n",
    "class My_Neural_Net(nn.Module):\n",
    "    #put in pass to avoid errors\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Linear(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the point we have the laid the ground work. The network is defined. Now we must tell PyTorch what going forward through our network means. Here is what it means in psuedocode\n",
    "\n",
    "    1. input X\n",
    "    2. linearly transform X into hidden data 1 via weights\n",
    "    3. perform ReLU on hidden data\n",
    "    4. linearly transform hidden data into hidden data 2 via weights\n",
    "    5. perform ReLU on hidden data\n",
    "    6. linearly transform hidden data into output layer via weights\n",
    "    7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "    8. output f(X) which should approximate y\n",
    "    \n",
    "### In PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1 via weights\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2 via weights\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer via weights\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now this needs to go into our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Liner(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1 via weights\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2 via weights\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer via weights\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss: Now we need to define how PyTorch knows how good our predictions are. Aka our loss at each iteration.\n",
    "\n",
    "* For a classification task you will typically use cross-entropy loss. However, you can define any loss function you want.\n",
    "* For example, maybe you want to modify cross entropy loss to weight false positives higher than false negatives or vice versa. You could do that.\n",
    "* For the popular loss functions, such as vanilla cross entropy loss, and mean square error, there is already a PyTorch function for it. These functions are highly optimized so it is reccomended to use them when they exist.\n",
    "\n",
    "<img src=\"data/cross_entropy.png\">\n",
    "\n",
    "\n",
    "\n",
    "* cross entropy loss is very high when true label is a 1 but predicted label is a 0 and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the loss into our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Liner(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In other frameworks, now would be the time where you need to define a backwards pass through your network. \n",
    "* Not in PyTorch. \n",
    "* Once we define forward, PyTorch can automatically computer derivates so we don't need to caculate the formulas by hand.\n",
    "\n",
    "### Essentially: We don't need a `backward()` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we do need now is a function that tells PyTorch how to fit the network. Here is the psuedocode\n",
    "    \n",
    "    1. input: N - number of iterations to train, X - data, y - target\n",
    "    2. for n going from 0 to N - 1:\n",
    "        3. f(X) = forward(X)\n",
    "        4. l = loss(f(X),y)\n",
    "        5. update weights in model to decrease loss and make f(X) closer to y\n",
    "  \n",
    "* How to update those weights is done by back propogation and gradient descent and outside the scope of this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a fit function\n",
    "\n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y,N = 5000):\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "            \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            \n",
    "            #print loss\n",
    "            print(l)\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without any early stopping/dropout/regularization you can greatly overfit but cannot fit everything into one lecture.\n",
    "\n",
    "# Lets put the fit function into our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Linear(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)\n",
    "    \n",
    "\n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y,N = 5000):\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "            \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            \n",
    "            #print loss\n",
    "            print(l)\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What function are we missing?\n",
    "* Once we've fit our model we probably want to use it to predict\n",
    "    * Ex: Predict whether a document is about baseball.\n",
    "* In practice, we typically split this into 2 functions.\n",
    "1. `predict_proba` outputs probabilities\n",
    "    * Our forward function outputs a vector probabilities of being a 1. For compatability with `sklearn` functions its a good idea to output a matrix where the first column is probability of being a `0` and second column is probability of being a `1`. Easy to create this as first and second column must add to 1 in each row.\n",
    "2. `predict` outputs class values\n",
    "    * generally, if `predict_proba` for an example (for being a `1`) outputs something `0.5 or greater` we say that we predicted a `1` and a `0` otherwise.\n",
    "    \n",
    "### Writing `predict_proba` and `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_proba(self, X):\n",
    "        # probability of being a 1\n",
    "        prob_1 = self.forward(X)\n",
    "              \n",
    "        # vectorwise subtraction\n",
    "        prob_0 = 1 - prob_1\n",
    "        \n",
    "        # make into a matrix\n",
    "        probs = torch.cat((prob_0,prob_1), dim = 1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        \n",
    "        # get only second column (probability of being a 1)\n",
    "        probs_1 = probs[:,1:]\n",
    "        \n",
    "        # 1 if prob_1 is greater or equal to than 0.5 for a given example\n",
    "        # 0 if less than 0.5\n",
    "        preds = (probs_1 >= 0.5).int()\n",
    "        \n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, add to class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Linear(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)\n",
    "    \n",
    "\n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y,N = 5000):\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "            \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            #print loss\n",
    "            print(l)\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # probability of being a 1\n",
    "        prob_1 = self.forward(X)\n",
    "              \n",
    "        # vectorwise subtraction\n",
    "        prob_0 = 1 - prob_1\n",
    "        \n",
    "        # make into a matrix\n",
    "        probs = torch.cat((prob_0,prob_1), dim = 1)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        \n",
    "        # get only second column (probability of being a 1)\n",
    "        probs_1 = probs[:,1:]\n",
    "        \n",
    "        # 1 if prob_1 is greater or equal to than 0.5 for a given example\n",
    "        # 0 if less than 0.5\n",
    "        preds = (probs_1 >= 0.5).int()\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One last function we may want to add (optional, but a good idea): A scoring function.\n",
    "### Tells us how we did after the whole process is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def score(self, X, y):\n",
    "        # proportion of times where we're correct\n",
    "        acc = (self.predict(X) == y).mean()\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(1000, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Linear(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)\n",
    "    \n",
    "\n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y,N = 5000):\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "            \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            #print loss\n",
    "            print(l)\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # probability of being a 1\n",
    "        prob_1 = self.forward(X)\n",
    "              \n",
    "        # vectorwise subtraction\n",
    "        prob_0 = 1 - prob_1\n",
    "        \n",
    "        # make into a matrix\n",
    "        probs = torch.cat((prob_0,prob_1), dim = 1)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        \n",
    "        # get only second column (probability of being a 1)\n",
    "        probs_1 = probs[:,1:]\n",
    "        \n",
    "        # 1 if prob_1 is greater or equal to than 0.5 for a given example\n",
    "        # 0 if less than 0.5\n",
    "        preds = (probs_1 >= 0.5).int()\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # proportion of times where we're correct\n",
    "        # conversions just allow the math to work\n",
    "        acc = (self.predict(X) == y.int()).float().mean()\n",
    "        \n",
    "        return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting our Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our neural net\n",
    "neur_net = My_Neural_Net()\n",
    "\n",
    "# Split into train and test so we can fit on some data and see performance \n",
    "# on data we havent seen yet.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Turn X and y (train and test) into PyTorch objects. We always have to do this step\n",
    "X_train_tens = Variable(torch.Tensor(X_train).float())\n",
    "X_test_tens = Variable(torch.Tensor(X_test).float())\n",
    "y_train_tens = Variable(torch.Tensor(y_train).float())\n",
    "y_test_tens = Variable(torch.Tensor(y_test).float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neur_net.fit(X_train_tens,y_train_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neur_net.score(X_train_tens,y_train_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are overfitting, but still good score!\n",
    "# There are ways to correct overfitting.\n",
    "neur_net.score(X_test_tens,y_test_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One last thing:\n",
    "* At the very beggining we specificed that our first layer took the 1000 dimensional input and converted it to the hidden data which was 5 dimensional.\n",
    "* What if we didnt want to have to look at the dimensions of our input data every time?\n",
    "* We can do that in our constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructor\n",
    "    #take in X as a parameter\n",
    "    def __init__(self, X):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        #Find dimensionality of X\n",
    "        X_dim = X.shape[1]\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is X_dim dimensions\n",
    "        self.layer_1 = nn.Linear(X_dim, 4)\n",
    "\n",
    "# Thats all we have to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module): \n",
    "    \n",
    "    #constructor\n",
    "    #take in X as a parameter\n",
    "    def __init__(self, X):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        #Find dimensionality of X\n",
    "        X_dim = X.shape[1]\n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is X_dim dimensions\n",
    "        self.layer_1 = nn.Linear(X_dim, 4)\n",
    "        self.layer_2 = nn.Linear(4,4)\n",
    "        self.layer_3 = nn.Linear(4,1)\n",
    "\n",
    "        # Define activation functions. I'll be using ReLU\n",
    "        # for the hidden layers. Must use sigmoid for the \n",
    "        # final layer so we get a number between 0 and 1 for \n",
    "        # the probability of being about baseball.\n",
    "        # Luckily PyTorch already has ReLU and \n",
    "        # sigmoid.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.sigmoid(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.BCELoss()\n",
    "        return score(pred, true)\n",
    "    \n",
    "\n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y,N = 5000):\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "            \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            #print loss\n",
    "            print(l)\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # probability of being a 1\n",
    "        prob_1 = self.forward(X)\n",
    "              \n",
    "        # vectorwise subtraction\n",
    "        prob_0 = 1 - prob_1\n",
    "        \n",
    "        # make into a matrix\n",
    "        probs = torch.cat((prob_0,prob_1), dim = 1)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        \n",
    "        # get only second column (probability of being a 1)\n",
    "        probs_1 = probs[:,1:]\n",
    "        \n",
    "        # 1 if prob_1 is greater or equal to than 0.5 for a given example\n",
    "        # 0 if less than 0.5\n",
    "        preds = (probs_1 >= 0.5).int()\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # proportion of times where we're correct\n",
    "        # conversions just allow the math to work\n",
    "        acc = (self.predict(X) == y.int()).float().mean()\n",
    "        \n",
    "        return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only difference now is when we make our instance we need to pass in our X data.\n",
    "\n",
    "neur_net = My_Neural_Net(X_train_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything else the same\n",
    "\n",
    "neur_net.fit(X_train_tens,y_train_tens)\n",
    "neur_net.score(X_test_tens,y_test_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let Dr. Cook or me know if you're interested in:\n",
    "\n",
    "### More advanced usage:\n",
    "* Early stopping\n",
    "* Regularization\n",
    "* Dropout\n",
    "* Batch training\n",
    "* Etc.\n",
    "\n",
    "### More complicated networks\n",
    "* RNNs/LSTMs\n",
    "* CNNs\n",
    "* Others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tutorial]",
   "language": "python",
   "name": "conda-env-Tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
